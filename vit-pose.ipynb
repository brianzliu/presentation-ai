{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b048125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    RTDetrForObjectDetection,\n",
    "    VitPoseForPoseEstimation,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0ec513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Stage 1. Detect humans on the image\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# You can choose detector by your choice\n",
    "person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n",
    "person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe018458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Stage 2. Detect keypoints for each person found\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n",
    "model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f92c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_video_pose_estimation(input_video_path, output_video_path, detection_threshold=0.3, pose_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process a video file for pose estimation and overlay keypoints on each frame\n",
    "    \n",
    "    Args:\n",
    "        input_video_path: Path to input video file\n",
    "        output_video_path: Path to save output video\n",
    "        detection_threshold: Confidence threshold for person detection\n",
    "        pose_threshold: Confidence threshold for pose keypoints\n",
    "    \"\"\"\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "    \n",
    "    # Define codec and create VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Define colors for different people (BGR format for OpenCV)\n",
    "    colors = [\n",
    "        (0, 0, 255),      # Red\n",
    "        (255, 0, 0),      # Blue\n",
    "        (0, 255, 0),      # Green\n",
    "        (0, 255, 255),    # Yellow\n",
    "        (255, 0, 255),    # Magenta\n",
    "        (255, 165, 0),    # Orange\n",
    "        (255, 192, 203),  # Pink\n",
    "        (165, 42, 42)     # Brown\n",
    "    ]\n",
    "    \n",
    "    # Define skeleton connections (COCO format)\n",
    "    skeleton = [\n",
    "        [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],  # legs\n",
    "        [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],         # torso and arms\n",
    "        [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],         # arms and head\n",
    "        [2, 4], [3, 5], [4, 6], [5, 7]                    # head to shoulders\n",
    "    ]\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Convert BGR to RGB for PIL\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Stage 1: Detect humans\n",
    "            inputs = person_image_processor(images=pil_image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = person_model(**inputs)\n",
    "            \n",
    "            results = person_image_processor.post_process_object_detection(\n",
    "                outputs, target_sizes=torch.tensor([(pil_image.height, pil_image.width)]), \n",
    "                threshold=detection_threshold\n",
    "            )\n",
    "            result = results[0]\n",
    "            \n",
    "            # Get person boxes\n",
    "            person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n",
    "            \n",
    "            if len(person_boxes) > 0:\n",
    "                person_boxes = person_boxes.cpu().numpy()\n",
    "                \n",
    "                # Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\n",
    "                person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n",
    "                person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n",
    "                \n",
    "                # Stage 2: Detect keypoints\n",
    "                inputs = image_processor(pil_image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                pose_results = image_processor.post_process_pose_estimation(\n",
    "                    outputs, boxes=[person_boxes], threshold=pose_threshold\n",
    "                )\n",
    "                image_pose_result = pose_results[0]\n",
    "                \n",
    "                # Draw poses on frame\n",
    "                frame = draw_poses_on_frame(frame, image_pose_result, person_boxes, \n",
    "                                          colors, skeleton, pose_threshold)\n",
    "            \n",
    "            # Write frame to output video\n",
    "            out.write(frame)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Video processing complete! Output saved to: {output_video_path}\")\n",
    "\n",
    "def draw_poses_on_frame(frame, pose_results, boxes, colors, skeleton, min_score=0.3):\n",
    "    \"\"\"\n",
    "    Draw pose keypoints and skeleton on a frame using OpenCV\n",
    "    \"\"\"\n",
    "    # Draw bounding boxes and keypoints for each person\n",
    "    for i, (person_pose, box) in enumerate(zip(pose_results, boxes)):\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        x, y, w, h = box.astype(int)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # Extract keypoints, labels, and scores\n",
    "        keypoints = person_pose[\"keypoints\"]\n",
    "        labels = person_pose[\"labels\"]\n",
    "        scores = person_pose[\"scores\"]\n",
    "        \n",
    "        # Create a mapping from label to keypoint for easy access\n",
    "        keypoint_dict = {}\n",
    "        for keypoint, label, score in zip(keypoints, labels, scores):\n",
    "            if score >= min_score:\n",
    "                keypoint_dict[label.item()] = (int(keypoint[0].item()), int(keypoint[1].item()), score.item())\n",
    "        \n",
    "        # Draw skeleton connections\n",
    "        for connection in skeleton:\n",
    "            start_idx, end_idx = connection\n",
    "            if start_idx in keypoint_dict and end_idx in keypoint_dict:\n",
    "                start_x, start_y, _ = keypoint_dict[start_idx]\n",
    "                end_x, end_y, _ = keypoint_dict[end_idx]\n",
    "                cv2.line(frame, (start_x, start_y), (end_x, end_y), color, 2)\n",
    "        \n",
    "        # Draw keypoints\n",
    "        for keypoint, label, score in zip(keypoints, labels, scores):\n",
    "            if score >= min_score:\n",
    "                x, y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "                # Size based on confidence score\n",
    "                radius = max(3, int(5 + (score.item() * 5)))\n",
    "                cv2.circle(frame, (x, y), radius, color, -1)\n",
    "                cv2.circle(frame, (x, y), radius, (255, 255, 255), 1)  # White border\n",
    "                \n",
    "                # Optionally add keypoint labels\n",
    "                keypoint_name = model.config.id2label[label.item()]\n",
    "                cv2.putText(frame, keypoint_name, (x + 5, y - 5), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "    \n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c80916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties: 1620x1080, 30 FPS, 317 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/317 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 317/317 [00:39<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete! Output saved to: output_video_with_poses.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage for video processing\n",
    "# Make sure to install required dependencies first:\n",
    "# pip install opencv-python tqdm matplotlib\n",
    "\n",
    "# Example 1: Process a video file\n",
    "input_video = \"input_video.mp4\"  # Replace with your video file path\n",
    "output_video = \"output_video_with_poses.mp4\"\n",
    "\n",
    "# Check if input video exists\n",
    "if os.path.exists(input_video):\n",
    "    # Process the video\n",
    "    process_video_pose_estimation(\n",
    "        input_video_path=input_video,\n",
    "        output_video_path=output_video,\n",
    "        detection_threshold=0.3,  # Confidence threshold for person detection\n",
    "        pose_threshold=0.3        # Confidence threshold for pose keypoints\n",
    "    )\n",
    "else:\n",
    "    print(f\"Video file {input_video} not found. Please provide a valid video file path.\")\n",
    "    \n",
    "# Example 2: Process a video from webcam (live processing)\n",
    "# Uncomment the following lines to process webcam feed:\n",
    "# process_video_pose_estimation(\n",
    "#     input_video_path=0,  # 0 for default webcam\n",
    "#     output_video_path=\"webcam_poses.mp4\",\n",
    "#     detection_threshold=0.3,\n",
    "#     pose_threshold=0.3\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open camera 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@53.655] global cap_v4l.cpp:913 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@53.655] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "def real_time_pose_estimation(camera_id=0, detection_threshold=0.3, pose_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Real-time pose estimation from webcam feed with live display\n",
    "    Press 'q' to quit\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open camera {camera_id}\")\n",
    "        return\n",
    "    \n",
    "    # Define colors for different people (BGR format for OpenCV)\n",
    "    colors = [\n",
    "        (0, 0, 255),      # Red\n",
    "        (255, 0, 0),      # Blue\n",
    "        (0, 255, 0),      # Green\n",
    "        (0, 255, 255),    # Yellow\n",
    "        (255, 0, 255),    # Magenta\n",
    "        (255, 165, 0),    # Orange\n",
    "        (255, 192, 203),  # Pink\n",
    "        (165, 42, 42)     # Brown\n",
    "    ]\n",
    "    \n",
    "    # Define skeleton connections (COCO format)\n",
    "    skeleton = [\n",
    "        [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],  # legs\n",
    "        [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],         # torso and arms\n",
    "        [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],         # arms and head\n",
    "        [2, 4], [3, 5], [4, 6], [5, 7]                    # head to shoulders\n",
    "    ]\n",
    "    \n",
    "    print(\"Real-time pose estimation started. Press 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert BGR to RGB for PIL\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Stage 1: Detect humans\n",
    "        inputs = person_image_processor(images=pil_image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = person_model(**inputs)\n",
    "        \n",
    "        results = person_image_processor.post_process_object_detection(\n",
    "            outputs, target_sizes=torch.tensor([(pil_image.height, pil_image.width)]), \n",
    "            threshold=detection_threshold\n",
    "        )\n",
    "        result = results[0]\n",
    "        \n",
    "        # Get person boxes\n",
    "        person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n",
    "        \n",
    "        if len(person_boxes) > 0:\n",
    "            person_boxes = person_boxes.cpu().numpy()\n",
    "            \n",
    "            # Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\n",
    "            person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n",
    "            person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n",
    "            \n",
    "            # Stage 2: Detect keypoints\n",
    "            inputs = image_processor(pil_image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            pose_results = image_processor.post_process_pose_estimation(\n",
    "                outputs, boxes=[person_boxes], threshold=pose_threshold\n",
    "            )\n",
    "            image_pose_result = pose_results[0]\n",
    "            \n",
    "            # Draw poses on frame\n",
    "            frame = draw_poses_on_frame(frame, image_pose_result, person_boxes, \n",
    "                                      colors, skeleton, pose_threshold)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('Real-time Pose Estimation', frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Real-time pose estimation stopped.\")\n",
    "\n",
    "# Example usage for real-time processing:\n",
    "# Uncomment the following line to start real-time pose estimation\n",
    "# real_time_pose_estimation(camera_id=0, detection_threshold=0.3, pose_threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee642fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
